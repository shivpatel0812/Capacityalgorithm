{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SccEH2AVPqCj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import logging\n",
        "import openai\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def build_exogenous_variable(capacity_data, library_hours):\n",
        "    capacity_data['timestamp'] = pd.to_datetime(capacity_data['timestamp'])\n",
        "    capacity_data.set_index('timestamp', inplace=True)\n",
        "    hours_dict = {}\n",
        "    for _, row in library_hours.iterrows():\n",
        "        date_str = row['date'].strftime('%Y-%m-%d')\n",
        "        open_dt = pd.to_datetime(date_str + ' ' + row['open_time'])\n",
        "        close_dt = pd.to_datetime(date_str + ' ' + row['close_time'])\n",
        "        if close_dt < open_dt:\n",
        "            close_dt += pd.Timedelta(days=1)\n",
        "        hours_dict[date_str] = (open_dt, close_dt)\n",
        "    is_open = []\n",
        "    for ts in capacity_data.index:\n",
        "        date_str = ts.strftime('%Y-%m-%d')\n",
        "        if date_str in hours_dict:\n",
        "            open_dt, close_dt = hours_dict[date_str]\n",
        "            if open_dt <= ts <= close_dt:\n",
        "                is_open.append(1)\n",
        "            else:\n",
        "                is_open.append(0)\n",
        "        else:\n",
        "            is_open.append(0)\n",
        "    capacity_data['is_open'] = is_open\n",
        "    return capacity_dat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_academic_features(capacity_data, academic_calendar):\n",
        "    academic_calendar['start_date'] = pd.to_datetime(academic_calendar['start_date'])\n",
        "    academic_calendar['end_date'] = pd.to_datetime(academic_calendar['end_date'])\n",
        "    period_types = academic_calendar['period_type'].unique()\n",
        "    for p_type in period_types:\n",
        "        col_name = f\"is_{p_type}\"\n",
        "        capacity_data[col_name] = 0\n",
        "    for _, row in academic_calendar.iterrows():\n",
        "        p_type = row['period_type']\n",
        "        col_name = f\"is_{p_type}\"\n",
        "        mask = (capacity_data.index >= row['start_date']) & (capacity_data.index <= row['end_date'])\n",
        "        capacity_data.loc[mask, col_name] = 1\n",
        "    return capacity_data"
      ],
      "metadata": {
        "id": "wbELHEbtPtVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gpt_adjustment(prediction_df, context):\n",
        "    openai.api_key = \"your_openai_api_key\"\n",
        "    predictions = prediction_df['forecast'].tolist()\n",
        "    prompt = (\n",
        "        f\"Given the following capacity forecasts: {predictions}, and the context: \"\n",
        "        f\"'{context}', provide adjusted capacity predictions considering surges or drops.\"\n",
        "    )\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.2\n",
        "    )\n",
        "    raw_text = response['choices'][0]['text'].strip()\n",
        "    adjusted_predictions_str = [x.strip() for x in raw_text.split(',') if x.strip()]\n",
        "    if len(adjusted_predictions_str) != len(predictions):\n",
        "        prediction_df['forecast_adjusted'] = predictions\n",
        "        logging.warning(\"GPT returned unexpected format; using original predictions.\")\n",
        "    else:\n",
        "        try:\n",
        "            adjusted_vals = [float(val) for val in adjusted_predictions_str]\n",
        "            prediction_df['forecast_adjusted'] = adjusted_vals\n",
        "        except ValueError:\n",
        "            prediction_df['forecast_adjusted'] = predictions\n",
        "            logging.warning(\"GPT returned non-numeric data; using original predictions.\")\n",
        "    return prediction_df"
      ],
      "metadata": {
        "id": "Hy0DwUCtPu4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_and_forecast_arima(capacity_data, exog_columns):\n",
        "    train_size = int(len(capacity_data) * 0.8)\n",
        "    train_df = capacity_data.iloc[:train_size]\n",
        "    test_df = capacity_data.iloc[train_size:]\n",
        "    train_endog = train_df['capacity']\n",
        "    train_exog = train_df[exog_columns]\n",
        "    test_exog = test_df[exog_columns]\n",
        "    model = SARIMAX(\n",
        "        endog=train_endog,\n",
        "        exog=train_exog,\n",
        "        order=(1, 1, 1),\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "    results = model.fit(disp=False)\n",
        "    forecast = results.predict(\n",
        "        start=test_df.index[0],\n",
        "        end=test_df.index[-1],\n",
        "        exog=test_exog,\n",
        "        typ='levels'\n",
        "    )\n",
        "    forecast_df = pd.DataFrame({\"timestamp\": test_df.index, \"forecast\": forecast})\n",
        "    forecast_df.set_index(\"timestamp\", inplace=True)\n",
        "    return forecast_df\n"
      ],
      "metadata": {
        "id": "WwEghBJLPwiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    capacity_data = pd.read_csv('capacity_data.csv')\n",
        "    library_hours = pd.read_csv('library_hours.csv')\n",
        "    library_hours['date'] = pd.to_datetime(library_hours['date'])\n",
        "    academic_calendar = pd.read_csv('academic_calendar.csv')\n",
        "    merged_df = build_exogenous_variable(capacity_data, library_hours)\n",
        "    merged_df = add_academic_features(merged_df, academic_calendar)\n",
        "    exog_cols = ['is_open'] + [col for col in merged_df.columns if col.startswith(\"is_\") and col != \"is_open\"]\n",
        "    forecast_df = train_and_forecast_arima(merged_df, exog_cols)\n",
        "    context_info = \"Clemons Library at UVA, Finals Week, historically higher capacity.\"\n",
        "    forecast_df = apply_gpt_adjustment(forecast_df, context_info)\n",
        "    forecast_df.to_csv('forecast_results.csv')\n",
        "    print(forecast_df.tail(10))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kSrI-RuEPx0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}